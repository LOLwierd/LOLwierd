<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>lolwierd</title>
<meta name="title" content="lolwierd" />
<meta name="description" content="Knobs Temperature: higher value -&gt; more deterministic output. lower value -&gt; more creative output (higher chances of hallucination?). essentially increasing the weights of other possible tokens! as temperature decreases. so obv for fact based questions -&gt; lower temp, for creative tasks -&gt; higher temp Top_p: controls how deterministic the model is at generating responses! treat almost the same way as temperature. The general recommendation is to alter one, not both." />
<meta name="keywords" content="" />


<meta property="og:title" content="" />
<meta property="og:description" content="Knobs Temperature: higher value -&gt; more deterministic output. lower value -&gt; more creative output (higher chances of hallucination?). essentially increasing the weights of other possible tokens! as temperature decreases. so obv for fact based questions -&gt; lower temp, for creative tasks -&gt; higher temp Top_p: controls how deterministic the model is at generating responses! treat almost the same way as temperature. The general recommendation is to alter one, not both." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://lolwierd.com/notes/prompt-eng/" /><meta property="og:image" content="https://lolwierd.com/media/share.png"/><meta property="article:section" content="notes" />

<meta property="og:site_name" content="getting better" />




<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://lolwierd.com/media/share.png"/>

<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Knobs Temperature: higher value -&gt; more deterministic output. lower value -&gt; more creative output (higher chances of hallucination?). essentially increasing the weights of other possible tokens! as temperature decreases. so obv for fact based questions -&gt; lower temp, for creative tasks -&gt; higher temp Top_p: controls how deterministic the model is at generating responses! treat almost the same way as temperature. The general recommendation is to alter one, not both."/>



<meta itemprop="name" content="">
<meta itemprop="description" content="Knobs Temperature: higher value -&gt; more deterministic output. lower value -&gt; more creative output (higher chances of hallucination?). essentially increasing the weights of other possible tokens! as temperature decreases. so obv for fact based questions -&gt; lower temp, for creative tasks -&gt; higher temp Top_p: controls how deterministic the model is at generating responses! treat almost the same way as temperature. The general recommendation is to alter one, not both.">

<meta itemprop="wordCount" content="691"><meta itemprop="image" content="https://lolwierd.com/media/share.png"/>
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
    body {
        font-family: Verdana, sans-serif;
        margin: auto;
        padding: 20px;
        max-width: 720px;
        text-align: left;
        background-color: #f9f9f9;
        word-wrap: break-word;
        overflow-wrap: break-word;
        line-height: 1.5;
        color: #444;
        scroll-behavior: smooth;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    strong,
    b {
        color: #1f1f1f;
    }

    a {
        color: #2e3abc;
    }

    .title {
        text-decoration: none;
        border: 0;
    }

    .title span {
        font-weight: 400;
    }

    nav a {
        margin-right: 10px;
    }

    textarea {
        width: 100%;
        font-size: 16px;
    }

    input {
        font-size: 16px;
    }

    content {
        line-height: 1.6;
    }

    table {
        width: 100%;
    }

    img {
        max-width: 100%;
    }

    code {
        padding: 2px 5px;
        background-color: #f2f2f2;
    }

    pre code {
        color: #1f1f1f;
        display: block;
        padding: 20px;
        white-space: pre-wrap;
        font-size: 14px;
        overflow-x: auto;
    }

    div.highlight pre {
        background-color: initial;
        color: initial;
    }

    div.highlight code {
        background-color: unset;
        color: unset;
    }

    blockquote {
        border-left: 3px solid navy;
        color: #737373;
        padding-left: 12px;
        font-style: italic;
        margin-inline-start: 12px;
        margin-inline-end: 12px;

    }

    footer {
        padding: 25px;
        text-align: center;
    }

    .helptext {
        color: #777;
        font-size: small;
    }

    .errorlist {
        color: #eba613;
        font-size: small;
    }

     
    ul.blog-posts {
        list-style-type: none;
        padding: unset;
    }

    ul.blog-posts li {
        display: flex;
    }

    ul.blog-posts li span {
        flex: 0 0 130px;
    }

    ul.blog-posts li a:visited {
        color: #8b6fcb;
    }

    @media (prefers-color-scheme: dark) {
        * {
            color-scheme: dark;
        }

        body {
            background-color: #000;
            color: #ddd;
        }

        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        strong,
        b {
            color: #eee;
        }

        a {
            color: #8cc2dd;
             
        }

        code {
            background-color: #222;
        }

        pre code {
            color: #ddd;
        }

        blockquote {
            color: #ccc;
        }

        textarea,
        input {
            background-color: #252525;
            color: #ddd;
        }

        .helptext {
            color: #aaa;
        }
    }

    a {
        text-decoration: none;
    }

    a[title="alt-link"] {
        color: #4CA751;
    }

    a[title="notes"] {
         
         
        color: #E26D5A;
    }

    ul:has(input[type=checkbox]) {
        list-style-type: none;
        padding-inline-start: 8px;
    }

     
</style>
<script type="text/javascript">
  (function(c,l,a,r,i,t,y){
      c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
      t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
      y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
  })(window, document, "clarity", "script", "jcg9gkbk3q");
</script></head>

<body>
  <header><a href="/" class="title">
  <h2>lolwierd</h2>
</a>
<nav><a href="/">home</a>

<a href="/blog/">blog</a>

<a href="/resources/">resources</a>

<a href="/reading/">reading</a>

<a href="/about/">about</a>


</nav>
</header>
  <main>

<content>
  <a href="#knobs" class="header-link">
    <h3 id="knobs">
        Knobs
    </h3>
</a>
<ul>
<li>Temperature: higher value -&gt; more deterministic output. lower value -&gt; more creative output (<em>higher chances of hallucination?</em>). <strong>essentially increasing the weights of other possible tokens!</strong> as temperature decreases. so obv <em><strong>for fact based questions -&gt; lower temp, for creative tasks -&gt; higher temp</strong></em></li>
<li>Top_p: controls how deterministic the model is at generating responses! treat almost the same way as temperature.</li>
</ul>
<p><strong>The general recommendation is to alter one, not both.</strong></p>
<p>moving ahead with stating the obvious. ðŸ™ƒ</p>
<p>parts of a prompt</p>
<ul>
<li>Instruction - a specific task or instruction you want the model to perform</li>
<li>Context - external information or additional context that can steer the model to better responses</li>
<li>Input Data - the input or question that we are interested to find a response for</li>
<li>Output Indicator - the type or format of the output.</li>
</ul>
<p>imma not write obv things now. waste of time.</p>
<p>seperating the prompt into instruction and input helps.<br>
be precise. concise. dont be clever.</p>
<pre tabindex="0"><code>Explain the concept prompt engineering. Keep the explanation short, only a few sentences, and don&#39;t be too descriptive.
</code></pre><p>vs<br>
<strong></p>
<pre tabindex="0"><code>Use 2-3 sentences to explain the concept of prompt engineering to a high school student.
</code></pre></strong>
<p>dont tell what not to do. tell what to do instead. (basically avoid using directly negative speech??)</p>
<pre tabindex="0"><code>The following is an agent that recommends movies to a customer. DO NOT ASK FOR INTERESTS. DO NOT ASK FOR PERSONAL INFORMATION.
Customer: Please recommend a movie based on my interests.
Agent: 
</code></pre><p>vs<br>
<strong></p>
<pre tabindex="0"><code>The following is an agent that recommends movies to a customer. The agent is responsible to recommend a movie from the top global trending movies. It should refrain from asking users for their preferences and avoid asking for personal information. If the agent doesn&#39;t have a movie to recommend, it should respond &#34;Sorry, couldn&#39;t find a movie to recommend today.&#34;.
Customer: Please recommend a movie based on my interests.
Agent:
</code></pre></strong>
<p>mfing llms can do all nlp tasks. gibe prompt and boom. prompt with examples perform better in most cases.</p>
<p>gibe examples in da prompt if need answer in a specific format.</p>
<pre tabindex="0"><code>Classify the text into neutral, negative or positive. 
Text: I think the vacation is okay.
Sentiment: neutral 
Text: I think the food was okay. 
Sentiment:
</code></pre><a href="#zero-shot-prompting" class="header-link">
    <h3 id="zero-shot-prompting">
        zero-shot prompting
    </h3>
</a>
<p>no context straight question. bigger llms are generally better at this. basically ask questions like you&rsquo;re googling</p>
<p>instruction tuning improves this in llms. improving this will lead to general acceptance of llms (personal opinion).</p>
<pre tabindex="0"><code>Classify the text into neutral, negative or positive. 

Text: I think the vacation is okay.
Sentiment:
</code></pre><a href="#few-shot-prompting" class="header-link">
    <h3 id="few-shot-prompting">
        few-shot prompting
    </h3>
</a>
<p>basically give a few examples to the llm on how to solve the problem.</p>
<p>enables in context learning. - ability of llms to learn new tasks given a few demonstrations.</p>
<p>try to steer the model to give answers you want (or get better performance), give context before asking question.</p>
<p>not great for reasoning problems, but pretty good for use in labelling data.</p>
<pre tabindex="0"><code>Q: &lt;Question&gt;?
A: &lt;Answer&gt;
Q: &lt;Question&gt;?
A: &lt;Answer&gt;
Q: &lt;Question&gt;?
A: &lt;Answer&gt;
Q: &lt;Question&gt;?
A:
</code></pre><pre tabindex="0"><code>This is awesome! // Positive
This is bad! // Negative
Wow that movie was rad! // Positive
What a horrible show! //
</code></pre><p>glean the structure not the specific formats.</p>
<p>cool thing is in modern llms something like this works too.</p>
<pre tabindex="0"><code>Positive This is awesome! 
This is bad! Negative
Wow that movie was rad!
Positive
What a horrible show! --
</code></pre><a href="#chain-of-thought-prompting" class="header-link">
    <h3 id="chain-of-thought-prompting">
        chain of thought prompting
    </h3>
</a>
<p><img src="/media/cot.webp" alt="Chain of Thought Prompting" title="chain of thought prompting"></p>
<p>Bascially improve the reasoning capablities lacking in few shot prompting.</p>
<p>extend the few shot examples with intermediary steps of reasoning.<br>
It boots llms capabilities at reasoning tasks.<br>
the authors claim that this is an emergent ability that arises with sufficiently large language models.<br>
<a href="https://arxiv.org/abs/2201.11903"  target="_blank" rel="nofollow noopener noreferrer" >paper</a>
</p>
<a href="#zero-shot-cot-prompting" class="header-link">
    <h3 id="zero-shot-cot-prompting">
        Zero shot COT prompting
    </h3>
</a>
<p><img src="/media/zero-cot.webp" alt="Zero shot COT prompting" title="zero-shot cot prompting"></p>
<p>basically zero shot but when reasoning fails, add &ldquo;Lets think about this step by step&rdquo; and it bolsters the models ability on reasoning tasks.<br>
<a href="https://arxiv.org/pdf/2205.11916.pdf"  target="_blank" rel="nofollow noopener noreferrer" >paper</a>
</p>
<a href="#auto-cot-prompting" class="header-link">
    <h3 id="auto-cot-prompting">
        Auto COT prompting
    </h3>
</a>
<p><img src="/media/auto-cot.webp" alt="Auto COT prompting" title="auto cot prompting"></p>
<p>says that zero-shot cot ain&rsquo;t that good. so what we do is we use zero-shot cot to generate few-shot examples and use cot to get the answer for the intended reasoning question.</p>
<p>the main point is diversity of questions used in few shot examples is very important to mitigate faulty reasoning.<br>
<a href="https://arxiv.org/pdf/2210.03493.pdf"  target="_blank" rel="nofollow noopener noreferrer" >paper</a>
</p>
<a href="#self-consistency" class="header-link">
    <h3 id="self-consistency">
        Self-Consistency
    </h3>
</a>

</content>
<p>
  
</p>

  </main>
  <footer></footer>

  </body>

</html>
